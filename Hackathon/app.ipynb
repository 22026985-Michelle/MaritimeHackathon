{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  fuel  fuel_category sulfur engine_type  \\\n",
      "0  HFO (outside ECA and prior to 2020)              1  2.70%  propulsion   \n",
      "1  HFO (outside ECA and prior to 2020)              1  2.70%  propulsion   \n",
      "2  HFO (outside ECA and prior to 2020)              1  2.70%  propulsion   \n",
      "3  HFO (outside ECA and prior to 2020)              1  2.70%  propulsion   \n",
      "4  HFO (outside ECA and prior to 2020)              1  2.70%  propulsion   \n",
      "\n",
      "   emission_tier model_years   nox  pm10  pm2_5    voc     sox   n2_O   ch4  \\\n",
      "0            0.0    Pre-2000  18.1  1.42   1.31  0.632  10.293  0.031  0.01   \n",
      "1            1.0   2000-2010  17.0  1.42   1.31  0.632  10.293  0.031  0.01   \n",
      "2            2.0   post 2010  15.3  1.42   1.31  0.632  10.293  0.031  0.01   \n",
      "3            0.0    Pre-2000  14.0  1.43   1.32  0.530  11.244  0.030  0.01   \n",
      "4            1.0   2000-2010  13.0  1.43   1.32  0.530  11.244  0.030  0.01   \n",
      "\n",
      "    sfoc  n_ox  bsfc engine_size  s_ox  n2_o  \n",
      "0  195.0   NaN   NaN         NaN   NaN   NaN  \n",
      "1  195.0   NaN   NaN         NaN   NaN   NaN  \n",
      "2  195.0   NaN   NaN         NaN   NaN   NaN  \n",
      "3  213.0   NaN   NaN         NaN   NaN   NaN  \n",
      "4  213.0   NaN   NaN         NaN   NaN   NaN  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'vessel_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3300\\494054092.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0memission_factors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpropulsion_factors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboiler_factors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauxiliary_factors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Display the combined data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_factors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Step 1: Merge AIS data with emission factors based on vessel type or ID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmerged_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mais_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_factors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"vessel_type\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Adjust 'vessel_type' as per your dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m# Step 2: Calculate total time in anchorage (hours)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m merged_data[\"anchorage_time_hours\"] = (pd.to_datetime(merged_data[\"departure_time\"]) -  \n\u001b[0;32m     21\u001b[0m                                        pd.to_datetime(merged_data[\"arrival_time\"])).dt.total_seconds() / 3600 \n",
      "\u001b[1;32mc:\\Users\\miche\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9839\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9840\u001b[0m     ) -> DataFrame:\n\u001b[0;32m   9841\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9843\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m   9844\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9845\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9846\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m ) -> DataFrame:\n\u001b[1;32m--> 148\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    733\u001b[0m         (\n\u001b[0;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;31m# to avoid incompatible dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\miche\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1778\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'vessel_type'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "# Load AIS data \n",
    "ais_data = pd.read_csv(\"ais_dataset.csv\")  # Replace with actual file path \n",
    "# Load each emission factor file \n",
    "propulsion_factors = pd.read_csv(\"propulsion_engine_emission_factors.csv\") \n",
    "boiler_factors = pd.read_csv(\"boiler_engine_emission_factors.csv\") \n",
    "auxiliary_factors = pd.read_csv(\"auxiliary_engine_emission_factors.csv\") \n",
    "# Add a column to identify engine type \n",
    "propulsion_factors[\"engine_type\"] = \"propulsion\" \n",
    "boiler_factors[\"engine_type\"] = \"boiler\" \n",
    "auxiliary_factors[\"engine_type\"] = \"auxiliary\" \n",
    "# Combine all emission factor files into one DataFrame \n",
    "emission_factors = pd.concat([propulsion_factors, boiler_factors, auxiliary_factors], ignore_index=True) \n",
    "# Display the combined data \n",
    "print(emission_factors.head()) \n",
    "# Step 1: Merge AIS data with emission factors based on vessel type or ID \n",
    "merged_data = ais_data.merge(emission_factors, on=\"vessel_type\")  # Adjust 'vessel_type' as per your dataset \n",
    "# Step 2: Calculate total time in anchorage (hours) \n",
    "merged_data[\"anchorage_time_hours\"] = (pd.to_datetime(merged_data[\"departure_time\"]) -  \n",
    "                                       pd.to_datetime(merged_data[\"arrival_time\"])).dt.total_seconds() / 3600 \n",
    "# Step 3: Calculate emissions for each vessel \n",
    "merged_data[\"emission_before_jit\"] = (merged_data[\"fuel_consumption_rate\"] *  \n",
    "                                      merged_data[\"anchorage_time_hours\"] *  \n",
    "                                      merged_data[\"emission_factor\"]) \n",
    "# Step 4: Aggregate emissions \n",
    "anc_before_jit = merged_data[\"emission_before_jit\"].sum() \n",
    "print(f\"Total CO2 emissions before JIT: {anc_before_jit} tonnes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merge: (0, 30)\n",
      "After auxiliary merge: (0, 43)\n",
      "After boiler merge: (0, 53)\n",
      "Empty DataFrame\n",
      "Columns: [imo, anc_before_jit]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "ais_data = pd.read_csv('ais_dataset.csv')\n",
    "aux_engine_factors = pd.read_csv('auxiliary_engine_emission_factors.csv')\n",
    "boiler_engine_factors = pd.read_csv('boiler_engine_emission_factors.csv')\n",
    "\n",
    "# Filter engine factors for matching fuel_category\n",
    "aux_engine_factors = aux_engine_factors[aux_engine_factors['fuel_category'] == 2]\n",
    "boiler_engine_factors = boiler_engine_factors[boiler_engine_factors['fuel_category'] == 2]\n",
    "\n",
    "# Step 1: Filter for anchorage events\n",
    "anchorage_data = ais_data[(ais_data['speed'] < 1) & (ais_data['anchorage'] == 1)].copy()\n",
    "\n",
    "# Step 2: Convert 'timestamp' to datetime and drop invalid rows\n",
    "anchorage_data['timestamp'] = pd.to_datetime(anchorage_data['timestamp'], errors='coerce')\n",
    "anchorage_data.dropna(subset=['timestamp'], inplace=True)\n",
    "\n",
    "# Filter for July\n",
    "july_data = anchorage_data[anchorage_data['timestamp'].dt.month == 7]\n",
    "\n",
    "# Step 3: Merge datasets on 'fuel_category'\n",
    "print(\"Before merge:\", july_data.shape)\n",
    "aux_merged = pd.merge(july_data, aux_engine_factors, on='fuel_category', how='left')\n",
    "print(\"After auxiliary merge:\", aux_merged.shape)\n",
    "boiler_merged = pd.merge(aux_merged, boiler_engine_factors, on='fuel_category', how='left')\n",
    "print(\"After boiler merge:\", boiler_merged.shape)\n",
    "\n",
    "# Step 4: Calculate emissions\n",
    "boiler_merged['activity_duration_hours'] = (\n",
    "    boiler_merged['timestamp'].diff().dt.total_seconds() / 3600\n",
    ")\n",
    "boiler_merged['aux_emissions'] = (\n",
    "    boiler_merged['ael'] * boiler_merged['activity_duration_hours'] * boiler_merged['bsfc_x']\n",
    ")\n",
    "boiler_merged['boiler_emissions'] = (\n",
    "    boiler_merged['abl'] * boiler_merged['activity_duration_hours'] * boiler_merged['bsfc_y']\n",
    ")\n",
    "boiler_merged['total_emissions'] = (\n",
    "    boiler_merged['aux_emissions'] + boiler_merged['boiler_emissions']\n",
    ")\n",
    "\n",
    "# Step 5: Aggregate emissions for July\n",
    "july_emissions = boiler_merged.groupby('imo')['total_emissions'].sum().reset_index()\n",
    "july_emissions.columns = ['imo', 'anc_before_jit']\n",
    "july_emissions['anc_before_jit'] /= 1000  # Convert grams to tonnes\n",
    "\n",
    "# Save the results\n",
    "print(july_emissions)\n",
    "july_emissions.to_csv('anc_before_jit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imo', 'mmsi', 'vessel_name', 'date_of_build', 'vessel_type', 'group',\n",
      "       'timestamp', 'lon', 'lat', 'nav_stat', 'speed', 'course', 'heading',\n",
      "       'fuel_category', 'main_engine_fuel_type', 'aux_engine_fuel_type',\n",
      "       'engine_type', 'berth', 'port_name', 'anchorage', 'terminal',\n",
      "       'maneuvering_zone', 'p', 'vref', 'sfc_me', 'sfc_ae', 'sfc_ab', 'ael',\n",
      "       'abl', 'distance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ais_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2024-07-28T00:23:32.000Z\n",
      "1    2024-07-28T00:25:02.000Z\n",
      "2    2024-07-28T00:30:01.000Z\n",
      "3    2024-07-28T00:32:51.000Z\n",
      "4    2024-07-28T00:35:02.000Z\n",
      "Name: timestamp, dtype: object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(ais_data['timestamp'].head())\n",
    "print(ais_data['timestamp'].dtype)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIS Data Columns: Index(['imo', 'mmsi', 'vessel_name', 'date_of_build', 'vessel_type', 'group',\n",
      "       'timestamp', 'lon', 'lat', 'nav_stat', 'speed', 'course', 'heading',\n",
      "       'fuel_category', 'main_engine_fuel_type', 'aux_engine_fuel_type',\n",
      "       'engine_type', 'berth', 'port_name', 'anchorage', 'terminal',\n",
      "       'maneuvering_zone', 'p', 'vref', 'sfc_me', 'sfc_ae', 'sfc_ab', 'ael',\n",
      "       'abl', 'distance'],\n",
      "      dtype='object')\n",
      "Auxiliary Engine Factors Columns: Index(['fuel', 'fuel_category', 'sulfur', 'engine_size', 'emission_tier',\n",
      "       'model_years', 'nox', 'pm10', 'pm2_5', 'voc', 's_ox', 'n2_o', 'ch4',\n",
      "       'bsfc'],\n",
      "      dtype='object')\n",
      "Boiler Engine Factors Columns: Index(['fuel', 'fuel_category', 'sulfur', 'n_ox', 'pm10', 'pm2_5', 'voc',\n",
      "       'sox', 'n2_O', 'ch4', 'bsfc'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"AIS Data Columns:\", ais_data.columns)\n",
    "print(\"Auxiliary Engine Factors Columns:\", aux_engine_factors.columns)\n",
    "print(\"Boiler Engine Factors Columns:\", boiler_engine_factors.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[1 2 3]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(ais_data['fuel_category'].unique())\n",
    "print(aux_engine_factors['fuel_category'].unique())\n",
    "print(boiler_engine_factors['fuel_category'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merge: (0, 30)\n",
      "After auxiliary merge: (0, 43)\n",
      "After boiler merge: (0, 53)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before merge:\", july_data.shape)\n",
    "aux_merged = pd.merge(july_data, aux_engine_factors, on='fuel_category', how='left')\n",
    "print(\"After auxiliary merge:\", aux_merged.shape)\n",
    "boiler_merged = pd.merge(aux_merged, boiler_engine_factors, on='fuel_category', how='left')\n",
    "print(\"After boiler merge:\", boiler_merged.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuel categories in July data: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Fuel categories in July data:\", july_data['fuel_category'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorage_data = ais_data[ais_data['anchorage'].notnull()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_data = anchorage_data[anchorage_data['timestamp'].dt.month == 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorage_data = ais_data[ais_data['anchorage'].notnull()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After auxiliary merge: (1471404, 43)\n",
      "After boiler merge: (1471404, 53)\n",
      "Final emissions data:\n",
      "       imo  anc_before_jit\n",
      "0  1013315   -17009.030660\n",
      "1  1014838    -6457.638222\n",
      "2  1015820    24836.123125\n",
      "3  1017775     6518.825733\n",
      "4  1018547    99971.276731\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Merge datasets on 'fuel_category'\n",
    "aux_engine_factors = pd.read_csv('auxiliary_engine_emission_factors.csv')\n",
    "boiler_engine_factors = pd.read_csv('boiler_engine_emission_factors.csv')\n",
    "\n",
    "# Filter engine factors for matching fuel_category\n",
    "aux_engine_factors = aux_engine_factors[aux_engine_factors['fuel_category'] == 2]\n",
    "boiler_engine_factors = boiler_engine_factors[boiler_engine_factors['fuel_category'] == 2]\n",
    "\n",
    "# Merge auxiliary engine factors\n",
    "aux_merged = pd.merge(july_data, aux_engine_factors, on='fuel_category', how='left')\n",
    "print(\"After auxiliary merge:\", aux_merged.shape)\n",
    "\n",
    "# Merge boiler engine factors\n",
    "boiler_merged = pd.merge(aux_merged, boiler_engine_factors, on='fuel_category', how='left')\n",
    "print(\"After boiler merge:\", boiler_merged.shape)\n",
    "\n",
    "# Step 4: Calculate emissions\n",
    "# Calculate duration between timestamps in hours\n",
    "boiler_merged['activity_duration_hours'] = (\n",
    "    boiler_merged['timestamp'].diff().dt.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# Auxiliary engine emissions: E = L_A * A * EF\n",
    "boiler_merged['aux_emissions'] = (\n",
    "    boiler_merged['ael'] * boiler_merged['activity_duration_hours'] * boiler_merged['bsfc_x']\n",
    ")\n",
    "\n",
    "# Boiler emissions: E = L_B * A * EF\n",
    "boiler_merged['boiler_emissions'] = (\n",
    "    boiler_merged['abl'] * boiler_merged['activity_duration_hours'] * boiler_merged['bsfc_y']\n",
    ")\n",
    "\n",
    "# Total emissions at anchorage\n",
    "boiler_merged['total_emissions'] = (\n",
    "    boiler_merged['aux_emissions'] + boiler_merged['boiler_emissions']\n",
    ")\n",
    "\n",
    "# Step 5: Aggregate emissions for July\n",
    "july_emissions = boiler_merged.groupby('imo')['total_emissions'].sum().reset_index()\n",
    "july_emissions.columns = ['imo', 'anc_before_jit']\n",
    "july_emissions['anc_before_jit'] /= 1000  # Convert grams to tonnes\n",
    "\n",
    "# Save the results to a CSV file\n",
    "print(\"Final emissions data:\")\n",
    "print(july_emissions.head())\n",
    "july_emissions.to_csv('anc_before_jit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
